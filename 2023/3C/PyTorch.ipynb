{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a ConvNet PyTorch\n",
    "\n",
    "In this notebook, you'll learn how to use the powerful PyTorch framework to specify a conv net architecture and train it on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this PyTorch business?\n",
    "\n",
    "We will use one of two popular deep learning frameworks, PyTorch.\n",
    "\n",
    "Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. When using a framework like PyTorch or TensorFlow you can harness the power of the GPU for your own custom neural network architectures without having to write CUDA code directly.\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will I learn PyTorch?\n",
    "\n",
    "If you've used Torch before, but are new to PyTorch, this tutorial might be of use: http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in Torch. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "We load the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "NUM_TRAIN = 49000\n",
    "NUM_VAL = 1000\n",
    "\n",
    "cifar10_train = dset.CIFAR10('./data/CIFAR10', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./data/CIFAR10', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./test_data/CIFAR10', train=False, download=True,\n",
    "                          transform=T.ToTensor())\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we're going to use a CPU-friendly datatype. Later, we'll switch to a datatype that will move all our computations to the GPU and measure the speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor # the CPU datatype\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Model\n",
    "\n",
    "### Some assorted tidbits\n",
    "\n",
    "Let's start by looking at a simple model. First, note that PyTorch operates on Tensors, which are n-dimensional arrays functionally analogous to numpy's ndarrays, with the additional feature that they can be used for computations on GPUs.\n",
    "\n",
    "We'll provide you with a Flatten function, which we explain here. Remember that our image data (and more relevantly, our intermediate feature maps) are initially N x C x H x W, where:\n",
    "* N is the number of datapoints\n",
    "* C is the number of channels\n",
    "* H is the height of the intermediate feature map in pixels\n",
    "* W is the height of the intermediate feature map in pixels\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, that needs spatial understanding of where the intermediate features are relative to each other. When we input  data into fully connected affine layers, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"Flatten\" operation to collapse the C x H x W values per representation into a single long vector. The Flatten function below first reads in the N, C, H, and W values from a given batch of data, and then returns a \"view\" of that data. \"View\" is analogous to numpy's \"reshape\" method: it reshapes x's dimensions to be N x ??, where ?? is allowed to be anything (in this case, it will be C x H x W, but we don't need to specify that explicitly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example model itself\n",
    "\n",
    "The first step to training your own model is defining its architecture.\n",
    "\n",
    "Here's an example of a convolutional neural network defined in PyTorch -- try to understand what each line is doing, remembering that each layer is composed upon the previous layer. We haven't trained anything yet - that'll come next - for now, we want you to understand how everything gets set up.  nn.Sequential is a container which applies each layer\n",
    "one after the other.\n",
    "\n",
    "In that example, you see 2D convolutional layers (Conv2d), ReLU activations, and fully-connected layers (Linear). You also see the Cross-Entropy loss function, and the Adam optimizer being used. \n",
    "\n",
    "Make sure you understand why the parameters of the Linear layer are 5408 and 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Here's where we define the architecture of the model... \n",
    "simple_model = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=7, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                Flatten(), # see above for explanation\n",
    "                nn.Linear(5408, 10), # affine layer\n",
    "              )\n",
    "\n",
    "# Set the type of all data in this model to be FloatTensor \n",
    "simple_model.type(dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(simple_model.parameters(), lr=1e-2) # lr sets the learning rate of the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports many other layer types, loss functions, and optimizers - you will experiment with these next. Here's the official API documentation for these (if any of the parameters used above were unclear, this resource will also be helpful). One note: what we call in the class \"spatial batch norm\" is called \"BatchNorm2D\" in PyTorch.\n",
    "\n",
    "* Layers: http://pytorch.org/docs/nn.html\n",
    "* Activations: http://pytorch.org/docs/nn.html#non-linear-activations\n",
    "* Loss functions: http://pytorch.org/docs/nn.html#loss-functions\n",
    "* Optimizers: http://pytorch.org/docs/optim.html#algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a specific model\n",
    "\n",
    "In this section, we're going to specify a model for you to construct. The goal here isn't to get good performance (that'll be next), but instead to get comfortable with understanding the PyTorch documentation and configuring your own model. \n",
    "\n",
    "Using the code provided above as guidance, and using the following PyTorch documentation, specify a model with the following architecture:\n",
    "\n",
    "* 7x7 Convolutional Layer with 32 filters and stride of 1\n",
    "* ReLU Activation Layer\n",
    "* Spatial Batch Normalization Layer\n",
    "* 2x2 Max Pooling layer with a stride of 2\n",
    "* Affine layer with 1024 output units\n",
    "* ReLU Activation Layer\n",
    "* Affine layer from 1024 input units to 10 outputs\n",
    "\n",
    "And finally, set up a **cross-entropy** loss function and the **RMSprop** learning rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fixed_model_base = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, (7, 7)),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.MaxPool2d((2, 2), stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 13 * 13, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 10),\n",
    ")\n",
    "\n",
    "fixed_model = fixed_model_base.type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're doing the right thing, use the following tool to check the dimensionality of your output (it should be 64 x 10, since our batches have size 64 and the output of the final affine layer should be 10, corresponding to our 10 classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we're going to feed a random batch into the model you defined and make sure the output is the right size\n",
    "x = torch.randn(64, 3, 32, 32).type(dtype)\n",
    "x_var = Variable(x.type(dtype)) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model(x_var)        # Feed it through the model! \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([64, 10]))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU!\n",
    "\n",
    "Now, we're going to switch the dtype of the model and our data to the GPU-friendly tensors, and see what happens... everything is the same, except we are casting our model and input tensors as this new dtype instead of the old one.\n",
    "\n",
    "If this returns false, or otherwise fails in a not-graceful way (i.e., with some error message), you may not have an NVIDIA GPU available on your machine. If you're running locally, we recommend you switch to Google Cloud and follow the instructions to set up a GPU there. If you're already on Google Cloud, something is wrong -- make sure you followed the instructions on how to request and use a GPU on your instance. If you did, post on Piazza or come to Office Hours so we can help you debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that CUDA is properly configured and you have a GPU available\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "fixed_model_gpu = copy.deepcopy(fixed_model_base).type(gpu_dtype)\n",
    "\n",
    "x_gpu = torch.randn(64, 3, 32, 32).type(gpu_dtype)\n",
    "x_var_gpu = Variable(x.type(gpu_dtype)) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model_gpu(x_var_gpu)        # Feed it through the model! \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([64, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to evaluate the performance of the forward pass running on the CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.03 ms ± 91.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "ans = fixed_model(x_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317 µs ± 11.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "torch.cuda.synchronize() # Make sure there are no pending GPU computations\n",
    "ans = fixed_model_gpu(x_var_gpu)        # Feed it through the model! \n",
    "torch.cuda.synchronize() # Make sure there are no pending GPU computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that even a simple forward pass like this is significantly faster on the GPU. So for the rest of the task, you should use the GPU datatype for your model and your tensors: as a reminder that is *torch.cuda.FloatTensor* (in our notebook here as *gpu_dtype*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model.\n",
    "\n",
    "Now that you've seen how to define a model and do a single forward pass of some data through it, let's  walk through how you'd actually train one whole epoch over your training data (using the simple_model we provided above).\n",
    "\n",
    "Make sure you understand how each PyTorch function used below corresponds to what you implemented in your custom neural network implementation.\n",
    "\n",
    "Note that because we are not resetting the weights anywhere below, if you run the cell multiple times, you are effectively training multiple epochs (so your performance should improve).\n",
    "\n",
    "First, set up an RMSprop optimizer (using a 1e-3 learning rate) and a cross-entropy loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(fixed_model_gpu.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 100, loss = 1.4427\n",
      "t = 200, loss = 1.4868\n",
      "t = 300, loss = 1.4596\n",
      "t = 400, loss = 1.3090\n",
      "t = 500, loss = 1.2705\n",
      "t = 600, loss = 1.3884\n",
      "t = 700, loss = 1.2291\n"
     ]
    }
   ],
   "source": [
    "# This sets the model in \"training\" mode. This is relevant for some layers that may have different behavior\n",
    "# in training mode vs testing mode, such as Dropout and BatchNorm. \n",
    "fixed_model_gpu.train()\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_train):\n",
    "    x_var = Variable(x.type(gpu_dtype))\n",
    "    y_var = Variable(y.type(gpu_dtype).long())\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = fixed_model_gpu(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.item()))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've seen how the training process works in PyTorch. To save you writing boilerplate code, we're providing the following helper functions to help you train for multiple epochs and check the accuracy of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x_var = Variable(x.type(gpu_dtype))\n",
    "            y_var = Variable(y.type(gpu_dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.item()))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_accuracy(model, loader):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(gpu_dtype))\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc\n",
    "    \n",
    "@torch.no_grad()\n",
    "def save_result(model, loader):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    result = []\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(gpu_dtype))\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        result.append(preds)\n",
    "\n",
    "    with open(\"./result\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(result, fp)\n",
    "\n",
    "# we will use this function to check accuracy\n",
    "@torch.no_grad()\n",
    "def check_accuracy_from_result(loader, file_path = \"./result\"):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    with open(file_path, \"rb\") as fp:   #Unpickling\n",
    "        result = pickle.load(fp)\n",
    "    i = 0\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(gpu_dtype))\n",
    "        num_correct += (result[i] == y).sum()\n",
    "        num_samples += result[i].size(0)\n",
    "        i += 1\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the model.\n",
    "\n",
    "Let's see the train and check_accuracy code in action -- feel free to use these methods when evaluating the models you develop below.\n",
    "\n",
    "You should get a training loss of around 1.2-1.4, and a validation accuracy of around 50-60%. As mentioned above, if you re-run the cells, you'll be training more epochs, so your performance will improve past these numbers.\n",
    "\n",
    "But don't worry about getting these numbers better -- this was just practice before you tackle designing your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 1.4196\n",
      "t = 200, loss = 1.4750\n",
      "t = 300, loss = 1.3851\n",
      "t = 400, loss = 1.2305\n",
      "t = 500, loss = 1.2215\n",
      "t = 600, loss = 1.2695\n",
      "t = 700, loss = 1.3076\n",
      "Checking accuracy on validation set\n",
      "Got 462 / 1000 correct (46.20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.462"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.random.manual_seed(12345)\n",
    "fixed_model_gpu.apply(reset)\n",
    "train(fixed_model_gpu, loss_fn, optimizer, num_epochs=1)\n",
    "check_accuracy(fixed_model_gpu, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget the validation set!\n",
    "\n",
    "And note that you can use the check_accuracy function to evaluate on either the test set or the validation set, by passing either **loader_test** or **loader_val** as the second argument to check_accuracy. You should not touch the test set until you have finished your architecture and hyperparameter tuning, and only run the test set once at the end to report a final value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a _great_ model on CIFAR-10!\n",
    "\n",
    "Now it's your job to experiment with architectures, hyperparameters, loss functions, and optimizers to train a model that achieves **>=70%** accuracy on the CIFAR-10 **validation** set. You can use the check_accuracy and train functions from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- **Number of filters**: Above we used 32 filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use Dropout.\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at least 70% accuracy on the validation set. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. \n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(\n",
    "                in_planes,\n",
    "                out_planes,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                groups=groups,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "        layers.extend(\n",
    "            [\n",
    "                # dw\n",
    "                ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            ]\n",
    "        )\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=10, width_mult=1.0, init_weights=True):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        # CIFAR10\n",
    "        inverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 1],  # Stride 2 -> 1 for CIFAR-10\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "        # END\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * max(1.0, width_mult))\n",
    "\n",
    "        # CIFAR10: stride 2 -> 1\n",
    "        features = [ConvBNReLU(3, input_channel, stride=1)]\n",
    "        # END\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(\n",
    "                    block(input_channel, output_channel, stride, expand_ratio=t)\n",
    "                )\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        if init_weights:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight, 0, 0.01)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def mobilenet_v2(progress=True, device=\"cpu\", weights=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV2 architecture from\n",
    "    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" <https://arxiv.org/abs/1801.04381>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        model = MobileNetV2(init_weights=False, **kwargs).to(device)\n",
    "        state_dict = torch.load(weights)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        model = MobileNetV2(**kwargs).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model here, and make sure the output of this cell is the accuracy of your best model on the \n",
    "# train, val, and test sets. Here's some code to get you started. The output of this cell should be the training\n",
    "# and validation accuracy on your best model (measured by validation accuracy).\n",
    "\n",
    "model = mobilenet_v2(device=\"cuda\", weights=\"mobilenet_v2.pt\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.3832\n",
      "t = 200, loss = 0.7197\n",
      "t = 300, loss = 0.5255\n",
      "t = 400, loss = 0.6075\n",
      "t = 500, loss = 0.5767\n",
      "t = 600, loss = 0.4849\n",
      "t = 700, loss = 0.7408\n",
      "Checking accuracy on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_195840/1113198050.py:28: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  x_var = Variable(x.type(gpu_dtype), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 785 / 1000 correct (78.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_195840/1113198050.py:44: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  x_var = Variable(x.type(gpu_dtype), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.4142\n",
      "t = 200, loss = 0.5470\n",
      "t = 300, loss = 0.5028\n",
      "t = 400, loss = 0.5026\n",
      "t = 500, loss = 0.4353\n",
      "t = 600, loss = 0.3986\n",
      "t = 700, loss = 0.7017\n",
      "Checking accuracy on validation set\n",
      "Got 796 / 1000 correct (79.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.3811\n",
      "t = 200, loss = 0.4639\n",
      "t = 300, loss = 0.4529\n",
      "t = 400, loss = 0.4949\n",
      "t = 500, loss = 0.3778\n",
      "t = 600, loss = 0.4164\n",
      "t = 700, loss = 0.6804\n",
      "Checking accuracy on validation set\n",
      "Got 825 / 1000 correct (82.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.3238\n",
      "t = 200, loss = 0.4452\n",
      "t = 300, loss = 0.4372\n",
      "t = 400, loss = 0.5271\n",
      "t = 500, loss = 0.3388\n",
      "t = 600, loss = 0.3559\n",
      "t = 700, loss = 0.7515\n",
      "Checking accuracy on validation set\n",
      "Got 831 / 1000 correct (83.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.2732\n",
      "t = 200, loss = 0.3544\n",
      "t = 300, loss = 0.3800\n",
      "t = 400, loss = 0.4320\n",
      "t = 500, loss = 0.4141\n",
      "t = 600, loss = 0.2565\n",
      "t = 700, loss = 0.5369\n",
      "Checking accuracy on validation set\n",
      "Got 839 / 1000 correct (83.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.2011\n",
      "t = 200, loss = 0.2582\n",
      "t = 300, loss = 0.3546\n",
      "t = 400, loss = 0.3401\n",
      "t = 500, loss = 0.3509\n",
      "t = 600, loss = 0.2884\n",
      "t = 700, loss = 0.5516\n",
      "Checking accuracy on validation set\n",
      "Got 851 / 1000 correct (85.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.3005\n",
      "t = 200, loss = 0.3045\n",
      "t = 300, loss = 0.3895\n",
      "t = 400, loss = 0.4179\n",
      "t = 500, loss = 0.4838\n",
      "t = 600, loss = 0.2478\n",
      "t = 700, loss = 0.5844\n",
      "Checking accuracy on validation set\n",
      "Got 855 / 1000 correct (85.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.2078\n",
      "t = 200, loss = 0.3860\n",
      "t = 300, loss = 0.3087\n",
      "t = 400, loss = 0.5262\n",
      "t = 500, loss = 0.3188\n",
      "t = 600, loss = 0.3412\n",
      "t = 700, loss = 0.5525\n",
      "Checking accuracy on validation set\n",
      "Got 847 / 1000 correct (84.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1985\n",
      "t = 200, loss = 0.3594\n",
      "t = 300, loss = 0.2828\n",
      "t = 400, loss = 0.3370\n",
      "t = 500, loss = 0.3833\n",
      "t = 600, loss = 0.2266\n",
      "t = 700, loss = 0.5447\n",
      "Checking accuracy on validation set\n",
      "Got 852 / 1000 correct (85.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1916\n",
      "t = 200, loss = 0.2604\n",
      "t = 300, loss = 0.2862\n",
      "t = 400, loss = 0.2787\n",
      "t = 500, loss = 0.2653\n",
      "t = 600, loss = 0.2016\n",
      "t = 700, loss = 0.7128\n",
      "Checking accuracy on validation set\n",
      "Got 850 / 1000 correct (85.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1855\n",
      "t = 200, loss = 0.2064\n",
      "t = 300, loss = 0.1501\n",
      "t = 400, loss = 0.2961\n",
      "t = 500, loss = 0.2602\n",
      "t = 600, loss = 0.1669\n",
      "t = 700, loss = 0.5369\n",
      "Checking accuracy on validation set\n",
      "Got 858 / 1000 correct (85.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1629\n",
      "t = 200, loss = 0.1931\n",
      "t = 300, loss = 0.2379\n",
      "t = 400, loss = 0.3011\n",
      "t = 500, loss = 0.2387\n",
      "t = 600, loss = 0.2592\n",
      "t = 700, loss = 0.5152\n",
      "Checking accuracy on validation set\n",
      "Got 856 / 1000 correct (85.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1879\n",
      "t = 200, loss = 0.2114\n",
      "t = 300, loss = 0.2131\n",
      "t = 400, loss = 0.3248\n",
      "t = 500, loss = 0.2092\n",
      "t = 600, loss = 0.2027\n",
      "t = 700, loss = 0.3703\n",
      "Checking accuracy on validation set\n",
      "Got 865 / 1000 correct (86.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1565\n",
      "t = 200, loss = 0.2998\n",
      "t = 300, loss = 0.1845\n",
      "t = 400, loss = 0.2769\n",
      "t = 500, loss = 0.3917\n",
      "t = 600, loss = 0.1952\n",
      "t = 700, loss = 0.3106\n",
      "Checking accuracy on validation set\n",
      "Got 863 / 1000 correct (86.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0957\n",
      "t = 200, loss = 0.1892\n",
      "t = 300, loss = 0.1927\n",
      "t = 400, loss = 0.2275\n",
      "t = 500, loss = 0.1825\n",
      "t = 600, loss = 0.0831\n",
      "t = 700, loss = 0.3963\n",
      "Checking accuracy on validation set\n",
      "Got 842 / 1000 correct (84.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1141\n",
      "t = 200, loss = 0.2144\n",
      "t = 300, loss = 0.2288\n",
      "t = 400, loss = 0.2598\n",
      "t = 500, loss = 0.2833\n",
      "t = 600, loss = 0.2244\n",
      "t = 700, loss = 0.4882\n",
      "Checking accuracy on validation set\n",
      "Got 852 / 1000 correct (85.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1765\n",
      "t = 200, loss = 0.1487\n",
      "t = 300, loss = 0.1877\n",
      "t = 400, loss = 0.2472\n",
      "t = 500, loss = 0.1696\n",
      "t = 600, loss = 0.1485\n",
      "t = 700, loss = 0.2721\n",
      "Checking accuracy on validation set\n",
      "Got 860 / 1000 correct (86.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1709\n",
      "t = 200, loss = 0.1879\n",
      "t = 300, loss = 0.1866\n",
      "t = 400, loss = 0.2758\n",
      "t = 500, loss = 0.2003\n",
      "t = 600, loss = 0.1656\n",
      "t = 700, loss = 0.4241\n",
      "Checking accuracy on validation set\n",
      "Got 876 / 1000 correct (87.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1126\n",
      "t = 200, loss = 0.1401\n",
      "t = 300, loss = 0.0525\n",
      "t = 400, loss = 0.2607\n",
      "t = 500, loss = 0.1754\n",
      "t = 600, loss = 0.2176\n",
      "t = 700, loss = 0.3174\n",
      "Checking accuracy on validation set\n",
      "Got 866 / 1000 correct (86.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1450\n",
      "t = 200, loss = 0.2078\n",
      "t = 300, loss = 0.1474\n",
      "t = 400, loss = 0.2197\n",
      "t = 500, loss = 0.1177\n",
      "t = 600, loss = 0.1284\n",
      "t = 700, loss = 0.2937\n",
      "Checking accuracy on validation set\n",
      "Got 857 / 1000 correct (85.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1670\n",
      "t = 200, loss = 0.1482\n",
      "t = 300, loss = 0.1203\n",
      "t = 400, loss = 0.2780\n",
      "t = 500, loss = 0.1869\n",
      "t = 600, loss = 0.0872\n",
      "t = 700, loss = 0.2978\n",
      "Checking accuracy on validation set\n",
      "Got 864 / 1000 correct (86.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1796\n",
      "t = 200, loss = 0.1506\n",
      "t = 300, loss = 0.1487\n",
      "t = 400, loss = 0.2211\n",
      "t = 500, loss = 0.1869\n",
      "t = 600, loss = 0.0775\n",
      "t = 700, loss = 0.3585\n",
      "Checking accuracy on validation set\n",
      "Got 850 / 1000 correct (85.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.2078\n",
      "t = 200, loss = 0.2227\n",
      "t = 300, loss = 0.1287\n",
      "t = 400, loss = 0.1736\n",
      "t = 500, loss = 0.1329\n",
      "t = 600, loss = 0.0820\n",
      "t = 700, loss = 0.2078\n",
      "Checking accuracy on validation set\n",
      "Got 857 / 1000 correct (85.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0911\n",
      "t = 200, loss = 0.1043\n",
      "t = 300, loss = 0.2573\n",
      "t = 400, loss = 0.1181\n",
      "t = 500, loss = 0.0860\n",
      "t = 600, loss = 0.1075\n",
      "t = 700, loss = 0.2142\n",
      "Checking accuracy on validation set\n",
      "Got 863 / 1000 correct (86.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1967\n",
      "t = 200, loss = 0.1005\n",
      "t = 300, loss = 0.1966\n",
      "t = 400, loss = 0.1542\n",
      "t = 500, loss = 0.1034\n",
      "t = 600, loss = 0.0842\n",
      "t = 700, loss = 0.3000\n",
      "Checking accuracy on validation set\n",
      "Got 853 / 1000 correct (85.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0451\n",
      "t = 200, loss = 0.1107\n",
      "t = 300, loss = 0.1350\n",
      "t = 400, loss = 0.0987\n",
      "t = 500, loss = 0.0918\n",
      "t = 600, loss = 0.1740\n",
      "t = 700, loss = 0.2021\n",
      "Checking accuracy on validation set\n",
      "Got 879 / 1000 correct (87.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1610\n",
      "t = 200, loss = 0.1186\n",
      "t = 300, loss = 0.1008\n",
      "t = 400, loss = 0.2328\n",
      "t = 500, loss = 0.1580\n",
      "t = 600, loss = 0.1653\n",
      "t = 700, loss = 0.3411\n",
      "Checking accuracy on validation set\n",
      "Got 862 / 1000 correct (86.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0375\n",
      "t = 200, loss = 0.1793\n",
      "t = 300, loss = 0.1528\n",
      "t = 400, loss = 0.1000\n",
      "t = 500, loss = 0.1385\n",
      "t = 600, loss = 0.0799\n",
      "t = 700, loss = 0.3255\n",
      "Checking accuracy on validation set\n",
      "Got 865 / 1000 correct (86.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1350\n",
      "t = 200, loss = 0.0674\n",
      "t = 300, loss = 0.1720\n",
      "t = 400, loss = 0.0667\n",
      "t = 500, loss = 0.0987\n",
      "t = 600, loss = 0.1869\n",
      "t = 700, loss = 0.3671\n",
      "Checking accuracy on validation set\n",
      "Got 858 / 1000 correct (85.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1279\n",
      "t = 200, loss = 0.0795\n",
      "t = 300, loss = 0.1325\n",
      "t = 400, loss = 0.1025\n",
      "t = 500, loss = 0.0812\n",
      "t = 600, loss = 0.0887\n",
      "t = 700, loss = 0.1324\n",
      "Checking accuracy on validation set\n",
      "Got 863 / 1000 correct (86.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0731\n",
      "t = 200, loss = 0.1030\n",
      "t = 300, loss = 0.1397\n",
      "t = 400, loss = 0.1115\n",
      "t = 500, loss = 0.0847\n",
      "t = 600, loss = 0.0952\n",
      "t = 700, loss = 0.3508\n",
      "Checking accuracy on validation set\n",
      "Got 869 / 1000 correct (86.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0836\n",
      "t = 200, loss = 0.1890\n",
      "t = 300, loss = 0.2400\n",
      "t = 400, loss = 0.2015\n",
      "t = 500, loss = 0.2729\n",
      "t = 600, loss = 0.0999\n",
      "t = 700, loss = 0.1521\n",
      "Checking accuracy on validation set\n",
      "Got 866 / 1000 correct (86.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0500\n",
      "t = 200, loss = 0.0960\n",
      "t = 300, loss = 0.1951\n",
      "t = 400, loss = 0.1261\n",
      "t = 500, loss = 0.2308\n",
      "t = 600, loss = 0.1000\n",
      "t = 700, loss = 0.2954\n",
      "Checking accuracy on validation set\n",
      "Got 881 / 1000 correct (88.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0267\n",
      "t = 200, loss = 0.0540\n",
      "t = 300, loss = 0.1837\n",
      "t = 400, loss = 0.0663\n",
      "t = 500, loss = 0.0515\n",
      "t = 600, loss = 0.0735\n",
      "t = 700, loss = 0.2993\n",
      "Checking accuracy on validation set\n",
      "Got 857 / 1000 correct (85.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0909\n",
      "t = 200, loss = 0.0958\n",
      "t = 300, loss = 0.0989\n",
      "t = 400, loss = 0.1422\n",
      "t = 500, loss = 0.1037\n",
      "t = 600, loss = 0.1148\n",
      "t = 700, loss = 0.2392\n",
      "Checking accuracy on validation set\n",
      "Got 869 / 1000 correct (86.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0911\n",
      "t = 200, loss = 0.1026\n",
      "t = 300, loss = 0.0915\n",
      "t = 400, loss = 0.2092\n",
      "t = 500, loss = 0.1506\n",
      "t = 600, loss = 0.1143\n",
      "t = 700, loss = 0.2360\n",
      "Checking accuracy on validation set\n",
      "Got 848 / 1000 correct (84.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0653\n",
      "t = 200, loss = 0.0623\n",
      "t = 300, loss = 0.2153\n",
      "t = 400, loss = 0.1782\n",
      "t = 500, loss = 0.1529\n",
      "t = 600, loss = 0.1123\n",
      "t = 700, loss = 0.1695\n",
      "Checking accuracy on validation set\n",
      "Got 876 / 1000 correct (87.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0766\n",
      "t = 200, loss = 0.1409\n",
      "t = 300, loss = 0.0744\n",
      "t = 400, loss = 0.1299\n",
      "t = 500, loss = 0.1856\n",
      "t = 600, loss = 0.1018\n",
      "t = 700, loss = 0.3544\n",
      "Checking accuracy on validation set\n",
      "Got 870 / 1000 correct (87.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0387\n",
      "t = 200, loss = 0.0969\n",
      "t = 300, loss = 0.1343\n",
      "t = 400, loss = 0.1169\n",
      "t = 500, loss = 0.1721\n",
      "t = 600, loss = 0.1994\n",
      "t = 700, loss = 0.2461\n",
      "Checking accuracy on validation set\n",
      "Got 880 / 1000 correct (88.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0476\n",
      "t = 200, loss = 0.0838\n",
      "t = 300, loss = 0.1164\n",
      "t = 400, loss = 0.1603\n",
      "t = 500, loss = 0.1397\n",
      "t = 600, loss = 0.0924\n",
      "t = 700, loss = 0.2973\n",
      "Checking accuracy on validation set\n",
      "Got 879 / 1000 correct (87.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1158\n",
      "t = 200, loss = 0.0354\n",
      "t = 300, loss = 0.1363\n",
      "t = 400, loss = 0.1224\n",
      "t = 500, loss = 0.1059\n",
      "t = 600, loss = 0.0327\n",
      "t = 700, loss = 0.1534\n",
      "Checking accuracy on validation set\n",
      "Got 871 / 1000 correct (87.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0521\n",
      "t = 200, loss = 0.0406\n",
      "t = 300, loss = 0.1302\n",
      "t = 400, loss = 0.0905\n",
      "t = 500, loss = 0.1827\n",
      "t = 600, loss = 0.0588\n",
      "t = 700, loss = 0.2533\n",
      "Checking accuracy on validation set\n",
      "Got 866 / 1000 correct (86.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1058\n",
      "t = 200, loss = 0.0557\n",
      "t = 300, loss = 0.2349\n",
      "t = 400, loss = 0.1446\n",
      "t = 500, loss = 0.1384\n",
      "t = 600, loss = 0.1499\n",
      "t = 700, loss = 0.3771\n",
      "Checking accuracy on validation set\n",
      "Got 881 / 1000 correct (88.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0327\n",
      "t = 200, loss = 0.0589\n",
      "t = 300, loss = 0.2570\n",
      "t = 400, loss = 0.0481\n",
      "t = 500, loss = 0.0943\n",
      "t = 600, loss = 0.1604\n",
      "t = 700, loss = 0.2538\n",
      "Checking accuracy on validation set\n",
      "Got 859 / 1000 correct (85.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0406\n",
      "t = 200, loss = 0.0703\n",
      "t = 300, loss = 0.1626\n",
      "t = 400, loss = 0.0489\n",
      "t = 500, loss = 0.0796\n",
      "t = 600, loss = 0.1230\n",
      "t = 700, loss = 0.2769\n",
      "Checking accuracy on validation set\n",
      "Got 875 / 1000 correct (87.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0528\n",
      "t = 200, loss = 0.2455\n",
      "t = 300, loss = 0.1933\n",
      "t = 400, loss = 0.1810\n",
      "t = 500, loss = 0.1933\n",
      "t = 600, loss = 0.1502\n",
      "t = 700, loss = 0.1918\n",
      "Checking accuracy on validation set\n",
      "Got 870 / 1000 correct (87.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0471\n",
      "t = 200, loss = 0.0775\n",
      "t = 300, loss = 0.1890\n",
      "t = 400, loss = 0.1150\n",
      "t = 500, loss = 0.1843\n",
      "t = 600, loss = 0.1492\n",
      "t = 700, loss = 0.2047\n",
      "Checking accuracy on validation set\n",
      "Got 861 / 1000 correct (86.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0610\n",
      "t = 200, loss = 0.1432\n",
      "t = 300, loss = 0.0906\n",
      "t = 400, loss = 0.0832\n",
      "t = 500, loss = 0.0776\n",
      "t = 600, loss = 0.0639\n",
      "t = 700, loss = 0.2602\n",
      "Checking accuracy on validation set\n",
      "Got 872 / 1000 correct (87.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1322\n",
      "t = 200, loss = 0.0931\n",
      "t = 300, loss = 0.1479\n",
      "t = 400, loss = 0.1014\n",
      "t = 500, loss = 0.0752\n",
      "t = 600, loss = 0.0553\n",
      "t = 700, loss = 0.2368\n",
      "Checking accuracy on validation set\n",
      "Got 880 / 1000 correct (88.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1044\n",
      "t = 200, loss = 0.0964\n",
      "t = 300, loss = 0.0703\n",
      "t = 400, loss = 0.1193\n",
      "t = 500, loss = 0.1798\n",
      "t = 600, loss = 0.1350\n",
      "t = 700, loss = 0.2488\n",
      "Checking accuracy on validation set\n",
      "Got 875 / 1000 correct (87.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0229\n",
      "t = 200, loss = 0.0952\n",
      "t = 300, loss = 0.1705\n",
      "t = 400, loss = 0.0717\n",
      "t = 500, loss = 0.1095\n",
      "t = 600, loss = 0.0852\n",
      "t = 700, loss = 0.1934\n",
      "Checking accuracy on validation set\n",
      "Got 867 / 1000 correct (86.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0687\n",
      "t = 200, loss = 0.1094\n",
      "t = 300, loss = 0.1806\n",
      "t = 400, loss = 0.0473\n",
      "t = 500, loss = 0.0782\n",
      "t = 600, loss = 0.2470\n",
      "t = 700, loss = 0.1674\n",
      "Checking accuracy on validation set\n",
      "Got 847 / 1000 correct (84.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0418\n",
      "t = 200, loss = 0.0901\n",
      "t = 300, loss = 0.1257\n",
      "t = 400, loss = 0.1422\n",
      "t = 500, loss = 0.1441\n",
      "t = 600, loss = 0.0641\n",
      "t = 700, loss = 0.2326\n",
      "Checking accuracy on validation set\n",
      "Got 863 / 1000 correct (86.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1451\n",
      "t = 200, loss = 0.0555\n",
      "t = 300, loss = 0.0932\n",
      "t = 400, loss = 0.1066\n",
      "t = 500, loss = 0.0943\n",
      "t = 600, loss = 0.0683\n",
      "t = 700, loss = 0.1677\n",
      "Checking accuracy on validation set\n",
      "Got 858 / 1000 correct (85.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1512\n",
      "t = 200, loss = 0.1342\n",
      "t = 300, loss = 0.1145\n",
      "t = 400, loss = 0.0292\n",
      "t = 500, loss = 0.0488\n",
      "t = 600, loss = 0.1865\n",
      "t = 700, loss = 0.2076\n",
      "Checking accuracy on validation set\n",
      "Got 872 / 1000 correct (87.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0481\n",
      "t = 200, loss = 0.1214\n",
      "t = 300, loss = 0.0663\n",
      "t = 400, loss = 0.1388\n",
      "t = 500, loss = 0.0799\n",
      "t = 600, loss = 0.1381\n",
      "t = 700, loss = 0.2216\n",
      "Checking accuracy on validation set\n",
      "Got 886 / 1000 correct (88.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0336\n",
      "t = 200, loss = 0.1739\n",
      "t = 300, loss = 0.1131\n",
      "t = 400, loss = 0.0823\n",
      "t = 500, loss = 0.1872\n",
      "t = 600, loss = 0.0497\n",
      "t = 700, loss = 0.1969\n",
      "Checking accuracy on validation set\n",
      "Got 859 / 1000 correct (85.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0379\n",
      "t = 200, loss = 0.1197\n",
      "t = 300, loss = 0.0822\n",
      "t = 400, loss = 0.0910\n",
      "t = 500, loss = 0.1106\n",
      "t = 600, loss = 0.0932\n",
      "t = 700, loss = 0.1787\n",
      "Checking accuracy on validation set\n",
      "Got 868 / 1000 correct (86.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0790\n",
      "t = 200, loss = 0.0361\n",
      "t = 300, loss = 0.0884\n",
      "t = 400, loss = 0.0401\n",
      "t = 500, loss = 0.1109\n",
      "t = 600, loss = 0.1537\n",
      "t = 700, loss = 0.2910\n",
      "Checking accuracy on validation set\n",
      "Got 873 / 1000 correct (87.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0521\n",
      "t = 200, loss = 0.0659\n",
      "t = 300, loss = 0.1834\n",
      "t = 400, loss = 0.0496\n",
      "t = 500, loss = 0.1319\n",
      "t = 600, loss = 0.0867\n",
      "t = 700, loss = 0.0987\n",
      "Checking accuracy on validation set\n",
      "Got 889 / 1000 correct (88.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0616\n",
      "t = 200, loss = 0.0917\n",
      "t = 300, loss = 0.1001\n",
      "t = 400, loss = 0.2991\n",
      "t = 500, loss = 0.0549\n",
      "t = 600, loss = 0.0603\n",
      "t = 700, loss = 0.1226\n",
      "Checking accuracy on validation set\n",
      "Got 866 / 1000 correct (86.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0385\n",
      "t = 200, loss = 0.0572\n",
      "t = 300, loss = 0.1388\n",
      "t = 400, loss = 0.1059\n",
      "t = 500, loss = 0.1266\n",
      "t = 600, loss = 0.1356\n",
      "t = 700, loss = 0.2662\n",
      "Checking accuracy on validation set\n",
      "Got 873 / 1000 correct (87.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0302\n",
      "t = 200, loss = 0.0624\n",
      "t = 300, loss = 0.0902\n",
      "t = 400, loss = 0.0706\n",
      "t = 500, loss = 0.1292\n",
      "t = 600, loss = 0.1657\n",
      "t = 700, loss = 0.2144\n",
      "Checking accuracy on validation set\n",
      "Got 865 / 1000 correct (86.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0729\n",
      "t = 200, loss = 0.1063\n",
      "t = 300, loss = 0.2189\n",
      "t = 400, loss = 0.0664\n",
      "t = 500, loss = 0.0535\n",
      "t = 600, loss = 0.2747\n",
      "t = 700, loss = 0.1771\n",
      "Checking accuracy on validation set\n",
      "Got 867 / 1000 correct (86.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0838\n",
      "t = 200, loss = 0.1148\n",
      "t = 300, loss = 0.0856\n",
      "t = 400, loss = 0.1867\n",
      "t = 500, loss = 0.1570\n",
      "t = 600, loss = 0.0934\n",
      "t = 700, loss = 0.3176\n",
      "Checking accuracy on validation set\n",
      "Got 862 / 1000 correct (86.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0705\n",
      "t = 200, loss = 0.0653\n",
      "t = 300, loss = 0.1817\n",
      "t = 400, loss = 0.0826\n",
      "t = 500, loss = 0.1841\n",
      "t = 600, loss = 0.2049\n",
      "t = 700, loss = 0.2342\n",
      "Checking accuracy on validation set\n",
      "Got 876 / 1000 correct (87.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0555\n",
      "t = 200, loss = 0.0706\n",
      "t = 300, loss = 0.1317\n",
      "t = 400, loss = 0.1155\n",
      "t = 500, loss = 0.0459\n",
      "t = 600, loss = 0.1519\n",
      "t = 700, loss = 0.2122\n",
      "Checking accuracy on validation set\n",
      "Got 877 / 1000 correct (87.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0463\n",
      "t = 200, loss = 0.0927\n",
      "t = 300, loss = 0.2005\n",
      "t = 400, loss = 0.1462\n",
      "t = 500, loss = 0.1221\n",
      "t = 600, loss = 0.0994\n",
      "t = 700, loss = 0.2166\n",
      "Checking accuracy on validation set\n",
      "Got 875 / 1000 correct (87.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0701\n",
      "t = 200, loss = 0.1370\n",
      "t = 300, loss = 0.0677\n",
      "t = 400, loss = 0.0589\n",
      "t = 500, loss = 0.1842\n",
      "t = 600, loss = 0.1835\n",
      "t = 700, loss = 0.2273\n",
      "Checking accuracy on validation set\n",
      "Got 870 / 1000 correct (87.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0446\n",
      "t = 200, loss = 0.1209\n",
      "t = 300, loss = 0.1431\n",
      "t = 400, loss = 0.1400\n",
      "t = 500, loss = 0.0628\n",
      "t = 600, loss = 0.1313\n",
      "t = 700, loss = 0.2035\n",
      "Checking accuracy on validation set\n",
      "Got 866 / 1000 correct (86.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0965\n",
      "t = 200, loss = 0.1566\n",
      "t = 300, loss = 0.0666\n",
      "t = 400, loss = 0.1568\n",
      "t = 500, loss = 0.0854\n",
      "t = 600, loss = 0.0475\n",
      "t = 700, loss = 0.3018\n",
      "Checking accuracy on validation set\n",
      "Got 879 / 1000 correct (87.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1299\n",
      "t = 200, loss = 0.0951\n",
      "t = 300, loss = 0.1712\n",
      "t = 400, loss = 0.1096\n",
      "t = 500, loss = 0.2212\n",
      "t = 600, loss = 0.1713\n",
      "t = 700, loss = 0.1939\n",
      "Checking accuracy on validation set\n",
      "Got 894 / 1000 correct (89.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0815\n",
      "t = 200, loss = 0.0946\n",
      "t = 300, loss = 0.0661\n",
      "t = 400, loss = 0.2078\n",
      "t = 500, loss = 0.0608\n",
      "t = 600, loss = 0.0731\n",
      "t = 700, loss = 0.1626\n",
      "Checking accuracy on validation set\n",
      "Got 893 / 1000 correct (89.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0625\n",
      "t = 200, loss = 0.0987\n",
      "t = 300, loss = 0.1965\n",
      "t = 400, loss = 0.0468\n",
      "t = 500, loss = 0.1190\n",
      "t = 600, loss = 0.0658\n",
      "t = 700, loss = 0.2553\n",
      "Checking accuracy on validation set\n",
      "Got 879 / 1000 correct (87.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0469\n",
      "t = 200, loss = 0.1333\n",
      "t = 300, loss = 0.0897\n",
      "t = 400, loss = 0.1286\n",
      "t = 500, loss = 0.1049\n",
      "t = 600, loss = 0.1132\n",
      "t = 700, loss = 0.1175\n",
      "Checking accuracy on validation set\n",
      "Got 853 / 1000 correct (85.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0766\n",
      "t = 200, loss = 0.1174\n",
      "t = 300, loss = 0.0699\n",
      "t = 400, loss = 0.2306\n",
      "t = 500, loss = 0.0374\n",
      "t = 600, loss = 0.0962\n",
      "t = 700, loss = 0.1841\n",
      "Checking accuracy on validation set\n",
      "Got 864 / 1000 correct (86.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0289\n",
      "t = 200, loss = 0.0991\n",
      "t = 300, loss = 0.0532\n",
      "t = 400, loss = 0.0598\n",
      "t = 500, loss = 0.1490\n",
      "t = 600, loss = 0.0613\n",
      "t = 700, loss = 0.1687\n",
      "Checking accuracy on validation set\n",
      "Got 864 / 1000 correct (86.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0517\n",
      "t = 200, loss = 0.0646\n",
      "t = 300, loss = 0.2259\n",
      "t = 400, loss = 0.1492\n",
      "t = 500, loss = 0.1255\n",
      "t = 600, loss = 0.0883\n",
      "t = 700, loss = 0.2566\n",
      "Checking accuracy on validation set\n",
      "Got 861 / 1000 correct (86.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1184\n",
      "t = 200, loss = 0.1915\n",
      "t = 300, loss = 0.1616\n",
      "t = 400, loss = 0.1656\n",
      "t = 500, loss = 0.1561\n",
      "t = 600, loss = 0.0974\n",
      "t = 700, loss = 0.1763\n",
      "Checking accuracy on validation set\n",
      "Got 882 / 1000 correct (88.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0596\n",
      "t = 200, loss = 0.1201\n",
      "t = 300, loss = 0.1404\n",
      "t = 400, loss = 0.1291\n",
      "t = 500, loss = 0.0747\n",
      "t = 600, loss = 0.1107\n",
      "t = 700, loss = 0.1849\n",
      "Checking accuracy on validation set\n",
      "Got 854 / 1000 correct (85.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0770\n",
      "t = 200, loss = 0.0714\n",
      "t = 300, loss = 0.2049\n",
      "t = 400, loss = 0.1279\n",
      "t = 500, loss = 0.0820\n",
      "t = 600, loss = 0.1092\n",
      "t = 700, loss = 0.3466\n",
      "Checking accuracy on validation set\n",
      "Got 870 / 1000 correct (87.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0428\n",
      "t = 200, loss = 0.0975\n",
      "t = 300, loss = 0.1205\n",
      "t = 400, loss = 0.0844\n",
      "t = 500, loss = 0.1277\n",
      "t = 600, loss = 0.0802\n",
      "t = 700, loss = 0.3525\n",
      "Checking accuracy on validation set\n",
      "Got 869 / 1000 correct (86.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1297\n",
      "t = 200, loss = 0.0500\n",
      "t = 300, loss = 0.1405\n",
      "t = 400, loss = 0.1201\n",
      "t = 500, loss = 0.1010\n",
      "t = 600, loss = 0.1726\n",
      "t = 700, loss = 0.2118\n",
      "Checking accuracy on validation set\n",
      "Got 858 / 1000 correct (85.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0192\n",
      "t = 200, loss = 0.0726\n",
      "t = 300, loss = 0.2266\n",
      "t = 400, loss = 0.1410\n",
      "t = 500, loss = 0.1463\n",
      "t = 600, loss = 0.1439\n",
      "t = 700, loss = 0.2008\n",
      "Checking accuracy on validation set\n",
      "Got 882 / 1000 correct (88.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0465\n",
      "t = 200, loss = 0.0997\n",
      "t = 300, loss = 0.0581\n",
      "t = 400, loss = 0.1005\n",
      "t = 500, loss = 0.0490\n",
      "t = 600, loss = 0.1055\n",
      "t = 700, loss = 0.1133\n",
      "Checking accuracy on validation set\n",
      "Got 865 / 1000 correct (86.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0616\n",
      "t = 200, loss = 0.1003\n",
      "t = 300, loss = 0.1058\n",
      "t = 400, loss = 0.0417\n",
      "t = 500, loss = 0.0885\n",
      "t = 600, loss = 0.0433\n",
      "t = 700, loss = 0.2084\n",
      "Checking accuracy on validation set\n",
      "Got 869 / 1000 correct (86.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1069\n",
      "t = 200, loss = 0.1349\n",
      "t = 300, loss = 0.1777\n",
      "t = 400, loss = 0.1077\n",
      "t = 500, loss = 0.1375\n",
      "t = 600, loss = 0.0621\n",
      "t = 700, loss = 0.1737\n",
      "Checking accuracy on validation set\n",
      "Got 873 / 1000 correct (87.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0748\n",
      "t = 200, loss = 0.1599\n",
      "t = 300, loss = 0.0657\n",
      "t = 400, loss = 0.1034\n",
      "t = 500, loss = 0.0657\n",
      "t = 600, loss = 0.0664\n",
      "t = 700, loss = 0.3582\n",
      "Checking accuracy on validation set\n",
      "Got 867 / 1000 correct (86.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0969\n",
      "t = 200, loss = 0.0964\n",
      "t = 300, loss = 0.1873\n",
      "t = 400, loss = 0.0752\n",
      "t = 500, loss = 0.1307\n",
      "t = 600, loss = 0.1503\n",
      "t = 700, loss = 0.2620\n",
      "Checking accuracy on validation set\n",
      "Got 860 / 1000 correct (86.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0130\n",
      "t = 200, loss = 0.0572\n",
      "t = 300, loss = 0.1515\n",
      "t = 400, loss = 0.0714\n",
      "t = 500, loss = 0.0758\n",
      "t = 600, loss = 0.0468\n",
      "t = 700, loss = 0.3085\n",
      "Checking accuracy on validation set\n",
      "Got 847 / 1000 correct (84.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1426\n",
      "t = 200, loss = 0.0693\n",
      "t = 300, loss = 0.0828\n",
      "t = 400, loss = 0.0773\n",
      "t = 500, loss = 0.1504\n",
      "t = 600, loss = 0.1885\n",
      "t = 700, loss = 0.1553\n",
      "Checking accuracy on validation set\n",
      "Got 882 / 1000 correct (88.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0874\n",
      "t = 200, loss = 0.0559\n",
      "t = 300, loss = 0.1650\n",
      "t = 400, loss = 0.0920\n",
      "t = 500, loss = 0.1438\n",
      "t = 600, loss = 0.1289\n",
      "t = 700, loss = 0.1597\n",
      "Checking accuracy on validation set\n",
      "Got 868 / 1000 correct (86.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0642\n",
      "t = 200, loss = 0.1352\n",
      "t = 300, loss = 0.0781\n",
      "t = 400, loss = 0.1197\n",
      "t = 500, loss = 0.1362\n",
      "t = 600, loss = 0.1569\n",
      "t = 700, loss = 0.1235\n",
      "Checking accuracy on validation set\n",
      "Got 862 / 1000 correct (86.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0991\n",
      "t = 200, loss = 0.0796\n",
      "t = 300, loss = 0.1498\n",
      "t = 400, loss = 0.0988\n",
      "t = 500, loss = 0.1232\n",
      "t = 600, loss = 0.1061\n",
      "t = 700, loss = 0.1887\n",
      "Checking accuracy on validation set\n",
      "Got 874 / 1000 correct (87.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0669\n",
      "t = 200, loss = 0.0869\n",
      "t = 300, loss = 0.2210\n",
      "t = 400, loss = 0.1095\n",
      "t = 500, loss = 0.1980\n",
      "t = 600, loss = 0.1156\n",
      "t = 700, loss = 0.0845\n",
      "Checking accuracy on validation set\n",
      "Got 856 / 1000 correct (85.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1088\n",
      "t = 200, loss = 0.1525\n",
      "t = 300, loss = 0.1777\n",
      "t = 400, loss = 0.0750\n",
      "t = 500, loss = 0.0890\n",
      "t = 600, loss = 0.0460\n",
      "t = 700, loss = 0.1761\n",
      "Checking accuracy on validation set\n",
      "Got 858 / 1000 correct (85.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.2445\n",
      "t = 200, loss = 0.1310\n",
      "t = 300, loss = 0.0487\n",
      "t = 400, loss = 0.1293\n",
      "t = 500, loss = 0.1435\n",
      "t = 600, loss = 0.0735\n",
      "t = 700, loss = 0.1093\n",
      "Checking accuracy on validation set\n",
      "Got 873 / 1000 correct (87.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1719\n",
      "t = 200, loss = 0.0408\n",
      "t = 300, loss = 0.1390\n",
      "t = 400, loss = 0.0519\n",
      "t = 500, loss = 0.0551\n",
      "t = 600, loss = 0.2036\n",
      "t = 700, loss = 0.1709\n",
      "Checking accuracy on validation set\n",
      "Got 888 / 1000 correct (88.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0770\n",
      "t = 200, loss = 0.0962\n",
      "t = 300, loss = 0.1779\n",
      "t = 400, loss = 0.1967\n",
      "t = 500, loss = 0.0735\n",
      "t = 600, loss = 0.1350\n",
      "t = 700, loss = 0.2076\n",
      "Checking accuracy on validation set\n",
      "Got 874 / 1000 correct (87.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.2028\n",
      "t = 200, loss = 0.1254\n",
      "t = 300, loss = 0.1365\n",
      "t = 400, loss = 0.0972\n",
      "t = 500, loss = 0.0975\n",
      "t = 600, loss = 0.0719\n",
      "t = 700, loss = 0.1904\n",
      "Checking accuracy on validation set\n",
      "Got 867 / 1000 correct (86.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0239\n",
      "t = 200, loss = 0.1429\n",
      "t = 300, loss = 0.0788\n",
      "t = 400, loss = 0.1922\n",
      "t = 500, loss = 0.0676\n",
      "t = 600, loss = 0.0609\n",
      "t = 700, loss = 0.2112\n",
      "Checking accuracy on validation set\n",
      "Got 877 / 1000 correct (87.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1332\n",
      "t = 200, loss = 0.0454\n",
      "t = 300, loss = 0.0963\n",
      "t = 400, loss = 0.0818\n",
      "t = 500, loss = 0.1916\n",
      "t = 600, loss = 0.0894\n",
      "t = 700, loss = 0.1695\n",
      "Checking accuracy on validation set\n",
      "Got 882 / 1000 correct (88.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0526\n",
      "t = 200, loss = 0.0718\n",
      "t = 300, loss = 0.1231\n",
      "t = 400, loss = 0.0941\n",
      "t = 500, loss = 0.1038\n",
      "t = 600, loss = 0.0814\n",
      "t = 700, loss = 0.1207\n",
      "Checking accuracy on validation set\n",
      "Got 871 / 1000 correct (87.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1028\n",
      "t = 200, loss = 0.0521\n",
      "t = 300, loss = 0.1459\n",
      "t = 400, loss = 0.1589\n",
      "t = 500, loss = 0.0876\n",
      "t = 600, loss = 0.1208\n",
      "t = 700, loss = 0.1969\n",
      "Checking accuracy on validation set\n",
      "Got 869 / 1000 correct (86.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0216\n",
      "t = 200, loss = 0.0406\n",
      "t = 300, loss = 0.0956\n",
      "t = 400, loss = 0.0948\n",
      "t = 500, loss = 0.0752\n",
      "t = 600, loss = 0.0672\n",
      "t = 700, loss = 0.2287\n",
      "Checking accuracy on validation set\n",
      "Got 873 / 1000 correct (87.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0804\n",
      "t = 200, loss = 0.1495\n",
      "t = 300, loss = 0.0350\n",
      "t = 400, loss = 0.0813\n",
      "t = 500, loss = 0.0902\n",
      "t = 600, loss = 0.0811\n",
      "t = 700, loss = 0.1720\n",
      "Checking accuracy on validation set\n",
      "Got 877 / 1000 correct (87.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1410\n",
      "t = 200, loss = 0.0670\n",
      "t = 300, loss = 0.0771\n",
      "t = 400, loss = 0.1938\n",
      "t = 500, loss = 0.2002\n",
      "t = 600, loss = 0.1542\n",
      "t = 700, loss = 0.1905\n",
      "Checking accuracy on validation set\n",
      "Got 875 / 1000 correct (87.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0893\n",
      "t = 200, loss = 0.0993\n",
      "t = 300, loss = 0.0362\n",
      "t = 400, loss = 0.2141\n",
      "t = 500, loss = 0.1362\n",
      "t = 600, loss = 0.0455\n",
      "t = 700, loss = 0.1907\n",
      "Checking accuracy on validation set\n",
      "Got 855 / 1000 correct (85.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0676\n",
      "t = 200, loss = 0.2236\n",
      "t = 300, loss = 0.1519\n",
      "t = 400, loss = 0.1299\n",
      "t = 500, loss = 0.0593\n",
      "t = 600, loss = 0.0687\n",
      "t = 700, loss = 0.1102\n",
      "Checking accuracy on validation set\n",
      "Got 874 / 1000 correct (87.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1092\n",
      "t = 200, loss = 0.0390\n",
      "t = 300, loss = 0.1060\n",
      "t = 400, loss = 0.1522\n",
      "t = 500, loss = 0.0872\n",
      "t = 600, loss = 0.1222\n",
      "t = 700, loss = 0.2036\n",
      "Checking accuracy on validation set\n",
      "Got 870 / 1000 correct (87.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0561\n",
      "t = 200, loss = 0.0858\n",
      "t = 300, loss = 0.0889\n",
      "t = 400, loss = 0.1693\n",
      "t = 500, loss = 0.0638\n",
      "t = 600, loss = 0.0765\n",
      "t = 700, loss = 0.1201\n",
      "Checking accuracy on validation set\n",
      "Got 874 / 1000 correct (87.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0604\n",
      "t = 200, loss = 0.0520\n",
      "t = 300, loss = 0.1682\n",
      "t = 400, loss = 0.1350\n",
      "t = 500, loss = 0.1230\n",
      "t = 600, loss = 0.0830\n",
      "t = 700, loss = 0.1322\n",
      "Checking accuracy on validation set\n",
      "Got 870 / 1000 correct (87.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0435\n",
      "t = 200, loss = 0.0580\n",
      "t = 300, loss = 0.1237\n",
      "t = 400, loss = 0.0868\n",
      "t = 500, loss = 0.1601\n",
      "t = 600, loss = 0.0546\n",
      "t = 700, loss = 0.0794\n",
      "Checking accuracy on validation set\n",
      "Got 869 / 1000 correct (86.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0556\n",
      "t = 200, loss = 0.1082\n",
      "t = 300, loss = 0.0503\n",
      "t = 400, loss = 0.0771\n",
      "t = 500, loss = 0.0682\n",
      "t = 600, loss = 0.1259\n",
      "t = 700, loss = 0.3106\n",
      "Checking accuracy on validation set\n",
      "Got 879 / 1000 correct (87.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0729\n",
      "t = 200, loss = 0.0684\n",
      "t = 300, loss = 0.0799\n",
      "t = 400, loss = 0.1286\n",
      "t = 500, loss = 0.1134\n",
      "t = 600, loss = 0.0992\n",
      "t = 700, loss = 0.1434\n",
      "Checking accuracy on validation set\n",
      "Got 871 / 1000 correct (87.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0155\n",
      "t = 200, loss = 0.0664\n",
      "t = 300, loss = 0.1486\n",
      "t = 400, loss = 0.0854\n",
      "t = 500, loss = 0.1350\n",
      "t = 600, loss = 0.1458\n",
      "t = 700, loss = 0.2110\n",
      "Checking accuracy on validation set\n",
      "Got 869 / 1000 correct (86.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0266\n",
      "t = 200, loss = 0.0662\n",
      "t = 300, loss = 0.2129\n",
      "t = 400, loss = 0.2237\n",
      "t = 500, loss = 0.0322\n",
      "t = 600, loss = 0.0351\n",
      "t = 700, loss = 0.2171\n",
      "Checking accuracy on validation set\n",
      "Got 864 / 1000 correct (86.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0496\n",
      "t = 200, loss = 0.0665\n",
      "t = 300, loss = 0.1422\n",
      "t = 400, loss = 0.0677\n",
      "t = 500, loss = 0.0518\n",
      "t = 600, loss = 0.0562\n",
      "t = 700, loss = 0.2950\n",
      "Checking accuracy on validation set\n",
      "Got 881 / 1000 correct (88.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0345\n",
      "t = 200, loss = 0.1389\n",
      "t = 300, loss = 0.0953\n",
      "t = 400, loss = 0.0880\n",
      "t = 500, loss = 0.0474\n",
      "t = 600, loss = 0.0909\n",
      "t = 700, loss = 0.1431\n",
      "Checking accuracy on validation set\n",
      "Got 871 / 1000 correct (87.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0294\n",
      "t = 200, loss = 0.0743\n",
      "t = 300, loss = 0.0816\n",
      "t = 400, loss = 0.0701\n",
      "t = 500, loss = 0.1048\n",
      "t = 600, loss = 0.0999\n",
      "t = 700, loss = 0.2439\n",
      "Checking accuracy on validation set\n",
      "Got 878 / 1000 correct (87.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0586\n",
      "t = 200, loss = 0.1063\n",
      "t = 300, loss = 0.1162\n",
      "t = 400, loss = 0.0885\n",
      "t = 500, loss = 0.1693\n",
      "t = 600, loss = 0.0507\n",
      "t = 700, loss = 0.1300\n",
      "Checking accuracy on validation set\n",
      "Got 875 / 1000 correct (87.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0359\n",
      "t = 200, loss = 0.1039\n",
      "t = 300, loss = 0.1032\n",
      "t = 400, loss = 0.2954\n",
      "t = 500, loss = 0.0591\n",
      "t = 600, loss = 0.0866\n",
      "t = 700, loss = 0.2158\n",
      "Checking accuracy on validation set\n",
      "Got 869 / 1000 correct (86.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0762\n",
      "t = 200, loss = 0.1204\n",
      "t = 300, loss = 0.0852\n",
      "t = 400, loss = 0.1133\n",
      "t = 500, loss = 0.1820\n",
      "t = 600, loss = 0.1457\n",
      "t = 700, loss = 0.1780\n",
      "Checking accuracy on validation set\n",
      "Got 863 / 1000 correct (86.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0157\n",
      "t = 200, loss = 0.1582\n",
      "t = 300, loss = 0.1280\n",
      "t = 400, loss = 0.1421\n",
      "t = 500, loss = 0.0651\n",
      "t = 600, loss = 0.0854\n",
      "t = 700, loss = 0.1314\n",
      "Checking accuracy on validation set\n",
      "Got 880 / 1000 correct (88.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1063\n",
      "t = 200, loss = 0.1073\n",
      "t = 300, loss = 0.2591\n",
      "t = 400, loss = 0.1000\n",
      "t = 500, loss = 0.0962\n",
      "t = 600, loss = 0.0732\n",
      "t = 700, loss = 0.1661\n",
      "Checking accuracy on validation set\n",
      "Got 874 / 1000 correct (87.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0334\n",
      "t = 200, loss = 0.1766\n",
      "t = 300, loss = 0.2298\n",
      "t = 400, loss = 0.1247\n",
      "t = 500, loss = 0.1563\n",
      "t = 600, loss = 0.0729\n",
      "t = 700, loss = 0.2025\n",
      "Checking accuracy on validation set\n",
      "Got 872 / 1000 correct (87.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1069\n",
      "t = 200, loss = 0.1932\n",
      "t = 300, loss = 0.1403\n",
      "t = 400, loss = 0.0478\n",
      "t = 500, loss = 0.0620\n",
      "t = 600, loss = 0.0736\n",
      "t = 700, loss = 0.2338\n",
      "Checking accuracy on validation set\n",
      "Got 877 / 1000 correct (87.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0388\n",
      "t = 200, loss = 0.1198\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     acc \u001b[38;5;241m=\u001b[39m check_accuracy(model, loader_val)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m acc \u001b[38;5;241m>\u001b[39m best_acc:\n",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m x_var \u001b[38;5;241m=\u001b[39m Variable(x\u001b[38;5;241m.\u001b[39mtype(gpu_dtype))\n\u001b[1;32m      7\u001b[0m y_var \u001b[38;5;241m=\u001b[39m Variable(y\u001b[38;5;241m.\u001b[39mtype(gpu_dtype)\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m----> 9\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(scores, y_var)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36mMobileNetV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    114\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[0;32m---> 45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py:2438\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2436\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2440\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    train(model, loss_fn, optimizer, num_epochs=1)\n",
    "    acc = check_accuracy(model, loader_val)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"mobilenet_v2.pt\")\n",
    "        save_result(model, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes=10, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        # CIFAR 10 (7, 7) to (1, 1)\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 1, 4096),\n",
    "            # nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfgs = {\n",
    "    \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"D\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "    \"E\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def _vgg(arch, cfg, batch_norm, pretrained, progress, device, **kwargs):\n",
    "    if pretrained:\n",
    "        kwargs[\"init_weights\"] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs).to(device)\n",
    "    if pretrained:\n",
    "        script_dir = os.path.dirname(__file__)\n",
    "        state_dict = torch.load(\n",
    "            script_dir + \"/state_dicts/\" + arch + \".pt\", map_location=device\n",
    "        )\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg11_bn(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg(\"vgg11_bn\", \"A\", True, pretrained, progress, device, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13_bn(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg(\"vgg13_bn\", \"B\", True, pretrained, progress, device, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16_bn(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg(\"vgg16_bn\", \"D\", True, pretrained, progress, device, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19_bn(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg(\"vgg19_bn\", \"E\", True, pretrained, progress, device, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg13_bn(device=\"cuda\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0230\n",
      "t = 200, loss = 0.1211\n",
      "t = 300, loss = 0.1371\n",
      "t = 400, loss = 0.0780\n",
      "t = 500, loss = 0.1532\n",
      "t = 600, loss = 0.1047\n",
      "t = 700, loss = 0.4613\n",
      "Checking accuracy on validation set\n",
      "Got 816 / 1000 correct (81.60)\n",
      "Starting epoch 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_195840/1113198050.py:28: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  x_var = Variable(x.type(gpu_dtype), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 100, loss = 0.0707\n",
      "t = 200, loss = 0.1477\n",
      "t = 300, loss = 0.1377\n",
      "t = 400, loss = 0.0671\n",
      "t = 500, loss = 0.0780\n",
      "t = 600, loss = 0.0729\n",
      "t = 700, loss = 0.0928\n",
      "Checking accuracy on validation set\n",
      "Got 817 / 1000 correct (81.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1816\n",
      "t = 200, loss = 0.1059\n",
      "t = 300, loss = 0.0261\n",
      "t = 400, loss = 0.0320\n",
      "t = 500, loss = 0.1462\n",
      "t = 600, loss = 0.1498\n",
      "t = 700, loss = 0.0717\n",
      "Checking accuracy on validation set\n",
      "Got 799 / 1000 correct (79.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1606\n",
      "t = 200, loss = 0.1005\n",
      "t = 300, loss = 0.0727\n",
      "t = 400, loss = 0.0988\n",
      "t = 500, loss = 0.2146\n",
      "t = 600, loss = 0.0446\n",
      "t = 700, loss = 0.2973\n",
      "Checking accuracy on validation set\n",
      "Got 830 / 1000 correct (83.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.2007\n",
      "t = 200, loss = 0.1592\n",
      "t = 300, loss = 0.0167\n",
      "t = 400, loss = 0.2360\n",
      "t = 500, loss = 0.0730\n",
      "t = 600, loss = 0.1849\n",
      "t = 700, loss = 0.1052\n",
      "Checking accuracy on validation set\n",
      "Got 831 / 1000 correct (83.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0580\n",
      "t = 200, loss = 0.0348\n",
      "t = 300, loss = 0.0865\n",
      "t = 400, loss = 0.1799\n",
      "t = 500, loss = 0.0217\n",
      "t = 600, loss = 0.0502\n",
      "t = 700, loss = 0.2309\n",
      "Checking accuracy on validation set\n",
      "Got 825 / 1000 correct (82.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1350\n",
      "t = 200, loss = 0.1392\n",
      "t = 300, loss = 0.0928\n",
      "t = 400, loss = 0.2273\n",
      "t = 500, loss = 0.0324\n",
      "t = 600, loss = 0.0900\n",
      "t = 700, loss = 0.1280\n",
      "Checking accuracy on validation set\n",
      "Got 818 / 1000 correct (81.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1189\n",
      "t = 200, loss = 0.0690\n",
      "t = 300, loss = 0.0792\n",
      "t = 400, loss = 0.0759\n",
      "t = 500, loss = 0.1320\n",
      "t = 600, loss = 0.1222\n",
      "t = 700, loss = 0.3550\n",
      "Checking accuracy on validation set\n",
      "Got 819 / 1000 correct (81.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0367\n",
      "t = 200, loss = 0.1722\n",
      "t = 300, loss = 0.1094\n",
      "t = 400, loss = 0.0476\n",
      "t = 500, loss = 0.2583\n",
      "t = 600, loss = 0.2603\n",
      "t = 700, loss = 0.2732\n",
      "Checking accuracy on validation set\n",
      "Got 832 / 1000 correct (83.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0351\n",
      "t = 200, loss = 0.1867\n",
      "t = 300, loss = 0.1392\n",
      "t = 400, loss = 0.1557\n",
      "t = 500, loss = 0.1132\n",
      "t = 600, loss = 0.0717\n",
      "t = 700, loss = 0.0561\n",
      "Checking accuracy on validation set\n",
      "Got 811 / 1000 correct (81.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0771\n",
      "t = 200, loss = 0.0487\n",
      "t = 300, loss = 0.1307\n",
      "t = 400, loss = 0.3197\n",
      "t = 500, loss = 0.0970\n",
      "t = 600, loss = 0.0718\n",
      "t = 700, loss = 0.1265\n",
      "Checking accuracy on validation set\n",
      "Got 811 / 1000 correct (81.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0358\n",
      "t = 200, loss = 0.1886\n",
      "t = 300, loss = 0.1614\n",
      "t = 400, loss = 0.0776\n",
      "t = 500, loss = 0.1265\n",
      "t = 600, loss = 0.1195\n",
      "t = 700, loss = 0.3563\n",
      "Checking accuracy on validation set\n",
      "Got 837 / 1000 correct (83.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0960\n",
      "t = 200, loss = 0.0845\n",
      "t = 300, loss = 0.0463\n",
      "t = 400, loss = 0.0340\n",
      "t = 500, loss = 0.1487\n",
      "t = 600, loss = 0.1868\n",
      "t = 700, loss = 0.1432\n",
      "Checking accuracy on validation set\n",
      "Got 822 / 1000 correct (82.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0780\n",
      "t = 200, loss = 0.1378\n",
      "t = 300, loss = 0.0338\n",
      "t = 400, loss = 0.0383\n",
      "t = 500, loss = 0.0916\n",
      "t = 600, loss = 0.0773\n",
      "t = 700, loss = 0.2657\n",
      "Checking accuracy on validation set\n",
      "Got 811 / 1000 correct (81.10)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0433\n",
      "t = 200, loss = 0.1463\n",
      "t = 300, loss = 0.1017\n",
      "t = 400, loss = 0.1448\n",
      "t = 500, loss = 0.1592\n",
      "t = 600, loss = 0.1340\n",
      "t = 700, loss = 0.1331\n",
      "Checking accuracy on validation set\n",
      "Got 802 / 1000 correct (80.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1914\n",
      "t = 200, loss = 0.2284\n",
      "t = 300, loss = 0.0587\n",
      "t = 400, loss = 0.1075\n",
      "t = 500, loss = 0.0567\n",
      "t = 600, loss = 0.0451\n",
      "t = 700, loss = 0.1771\n",
      "Checking accuracy on validation set\n",
      "Got 839 / 1000 correct (83.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0312\n",
      "t = 200, loss = 0.2293\n",
      "t = 300, loss = 0.2145\n",
      "t = 400, loss = 0.0732\n",
      "t = 500, loss = 0.1229\n",
      "t = 600, loss = 0.0942\n",
      "t = 700, loss = 0.0957\n",
      "Checking accuracy on validation set\n",
      "Got 840 / 1000 correct (84.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0724\n",
      "t = 200, loss = 0.2878\n",
      "t = 300, loss = 0.1044\n",
      "t = 400, loss = 0.0679\n",
      "t = 500, loss = 0.0800\n",
      "t = 600, loss = 0.0405\n",
      "t = 700, loss = 0.1042\n",
      "Checking accuracy on validation set\n",
      "Got 813 / 1000 correct (81.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1683\n",
      "t = 200, loss = 0.0773\n",
      "t = 300, loss = 0.0505\n",
      "t = 400, loss = 0.2480\n",
      "t = 500, loss = 0.1045\n",
      "t = 600, loss = 0.0774\n",
      "t = 700, loss = 0.2406\n",
      "Checking accuracy on validation set\n",
      "Got 835 / 1000 correct (83.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1470\n",
      "t = 200, loss = 0.1731\n",
      "t = 300, loss = 0.0638\n",
      "t = 400, loss = 0.1071\n",
      "t = 500, loss = 0.3216\n",
      "t = 600, loss = 0.0062\n",
      "t = 700, loss = 0.1055\n",
      "Checking accuracy on validation set\n",
      "Got 827 / 1000 correct (82.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0970\n",
      "t = 200, loss = 0.1204\n",
      "t = 300, loss = 0.0459\n",
      "t = 400, loss = 0.0726\n",
      "t = 500, loss = 0.0666\n",
      "t = 600, loss = 0.2786\n",
      "t = 700, loss = 0.0906\n",
      "Checking accuracy on validation set\n",
      "Got 828 / 1000 correct (82.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0439\n",
      "t = 200, loss = 0.3225\n",
      "t = 300, loss = 0.0934\n",
      "t = 400, loss = 0.1366\n",
      "t = 500, loss = 0.0748\n",
      "t = 600, loss = 0.1184\n",
      "t = 700, loss = 0.4351\n",
      "Checking accuracy on validation set\n",
      "Got 845 / 1000 correct (84.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1214\n",
      "t = 200, loss = 0.0714\n",
      "t = 300, loss = 0.0408\n",
      "t = 400, loss = 0.0073\n",
      "t = 500, loss = 0.1015\n",
      "t = 600, loss = 0.0397\n",
      "t = 700, loss = 0.1067\n",
      "Checking accuracy on validation set\n",
      "Got 838 / 1000 correct (83.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0877\n",
      "t = 200, loss = 0.3493\n",
      "t = 300, loss = 0.0663\n",
      "t = 400, loss = 0.1033\n",
      "t = 500, loss = 0.3358\n",
      "t = 600, loss = 0.3432\n",
      "t = 700, loss = 0.0467\n",
      "Checking accuracy on validation set\n",
      "Got 828 / 1000 correct (82.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0366\n",
      "t = 200, loss = 0.2801\n",
      "t = 300, loss = 0.0465\n",
      "t = 400, loss = 0.1758\n",
      "t = 500, loss = 0.1186\n",
      "t = 600, loss = 0.0936\n",
      "t = 700, loss = 0.2351\n",
      "Checking accuracy on validation set\n",
      "Got 846 / 1000 correct (84.60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_195840/1113198050.py:44: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  x_var = Variable(x.type(gpu_dtype), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0324\n",
      "t = 200, loss = 0.0983\n",
      "t = 300, loss = 0.2630\n",
      "t = 400, loss = 0.0982\n",
      "t = 500, loss = 0.0773\n",
      "t = 600, loss = 0.1608\n",
      "t = 700, loss = 0.1368\n",
      "Checking accuracy on validation set\n",
      "Got 813 / 1000 correct (81.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1348\n",
      "t = 200, loss = 0.0347\n",
      "t = 300, loss = 0.2344\n",
      "t = 400, loss = 0.0723\n",
      "t = 500, loss = 0.1838\n",
      "t = 600, loss = 0.1851\n",
      "t = 700, loss = 0.1526\n",
      "Checking accuracy on validation set\n",
      "Got 819 / 1000 correct (81.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0522\n",
      "t = 200, loss = 0.2095\n",
      "t = 300, loss = 0.0897\n",
      "t = 400, loss = 0.1291\n",
      "t = 500, loss = 0.1150\n",
      "t = 600, loss = 0.0161\n",
      "t = 700, loss = 0.2209\n",
      "Checking accuracy on validation set\n",
      "Got 822 / 1000 correct (82.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0559\n",
      "t = 200, loss = 0.1495\n",
      "t = 300, loss = 0.1610\n",
      "t = 400, loss = 0.2604\n",
      "t = 500, loss = 0.1273\n",
      "t = 600, loss = 0.1404\n",
      "t = 700, loss = 0.1324\n",
      "Checking accuracy on validation set\n",
      "Got 819 / 1000 correct (81.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1969\n",
      "t = 200, loss = 0.0665\n",
      "t = 300, loss = 0.1006\n",
      "t = 400, loss = 0.0943\n",
      "t = 500, loss = 0.0959\n",
      "t = 600, loss = 0.0268\n",
      "t = 700, loss = 0.1479\n",
      "Checking accuracy on validation set\n",
      "Got 820 / 1000 correct (82.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1180\n",
      "t = 200, loss = 0.0634\n",
      "t = 300, loss = 0.0784\n",
      "t = 400, loss = 0.0282\n",
      "t = 500, loss = 0.2209\n",
      "t = 600, loss = 0.1587\n",
      "t = 700, loss = 0.1545\n",
      "Checking accuracy on validation set\n",
      "Got 848 / 1000 correct (84.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1348\n",
      "t = 200, loss = 0.0414\n",
      "t = 300, loss = 0.1446\n",
      "t = 400, loss = 0.0885\n",
      "t = 500, loss = 0.0228\n",
      "t = 600, loss = 0.0917\n",
      "t = 700, loss = 0.0241\n",
      "Checking accuracy on validation set\n",
      "Got 824 / 1000 correct (82.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0484\n",
      "t = 200, loss = 0.2137\n",
      "t = 300, loss = 0.0898\n",
      "t = 400, loss = 0.0408\n",
      "t = 500, loss = 0.0617\n",
      "t = 600, loss = 0.0233\n",
      "t = 700, loss = 0.2470\n",
      "Checking accuracy on validation set\n",
      "Got 843 / 1000 correct (84.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.2390\n",
      "t = 200, loss = 0.1388\n",
      "t = 300, loss = 0.0886\n",
      "t = 400, loss = 0.0628\n",
      "t = 500, loss = 0.1452\n",
      "t = 600, loss = 0.0908\n",
      "t = 700, loss = 0.1743\n",
      "Checking accuracy on validation set\n",
      "Got 832 / 1000 correct (83.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0284\n",
      "t = 200, loss = 0.2860\n",
      "t = 300, loss = 0.0904\n",
      "t = 400, loss = 0.1928\n",
      "t = 500, loss = 0.2238\n",
      "t = 600, loss = 0.0791\n",
      "t = 700, loss = 0.0960\n",
      "Checking accuracy on validation set\n",
      "Got 817 / 1000 correct (81.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1066\n",
      "t = 200, loss = 0.1275\n",
      "t = 300, loss = 0.1462\n",
      "t = 400, loss = 0.0938\n",
      "t = 500, loss = 0.0222\n",
      "t = 600, loss = 0.0637\n",
      "t = 700, loss = 0.1486\n",
      "Checking accuracy on validation set\n",
      "Got 839 / 1000 correct (83.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0882\n",
      "t = 200, loss = 0.4013\n",
      "t = 300, loss = 0.1320\n",
      "t = 400, loss = 0.1711\n",
      "t = 500, loss = 0.0759\n",
      "t = 600, loss = 0.1750\n",
      "t = 700, loss = 0.2782\n",
      "Checking accuracy on validation set\n",
      "Got 833 / 1000 correct (83.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0294\n",
      "t = 200, loss = 0.1262\n",
      "t = 300, loss = 0.0739\n",
      "t = 400, loss = 0.0144\n",
      "t = 500, loss = 0.1481\n",
      "t = 600, loss = 0.0363\n",
      "t = 700, loss = 0.0806\n",
      "Checking accuracy on validation set\n",
      "Got 838 / 1000 correct (83.80)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0713\n",
      "t = 200, loss = 0.1992\n",
      "t = 300, loss = 0.1178\n",
      "t = 400, loss = 0.0902\n",
      "t = 500, loss = 0.2463\n",
      "t = 600, loss = 0.2280\n",
      "t = 700, loss = 0.1836\n",
      "Checking accuracy on validation set\n",
      "Got 846 / 1000 correct (84.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1441\n",
      "t = 200, loss = 0.0434\n",
      "t = 300, loss = 0.0411\n",
      "t = 400, loss = 0.0505\n",
      "t = 500, loss = 0.0138\n",
      "t = 600, loss = 0.0982\n",
      "t = 700, loss = 0.1634\n",
      "Checking accuracy on validation set\n",
      "Got 815 / 1000 correct (81.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1364\n",
      "t = 200, loss = 0.1033\n",
      "t = 300, loss = 0.0712\n",
      "t = 400, loss = 0.0370\n",
      "t = 500, loss = 0.2771\n",
      "t = 600, loss = 0.1310\n",
      "t = 700, loss = 0.0532\n",
      "Checking accuracy on validation set\n",
      "Got 830 / 1000 correct (83.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0824\n",
      "t = 200, loss = 0.0975\n",
      "t = 300, loss = 0.1395\n",
      "t = 400, loss = 0.0561\n",
      "t = 500, loss = 0.1460\n",
      "t = 600, loss = 0.1535\n",
      "t = 700, loss = 0.0925\n",
      "Checking accuracy on validation set\n",
      "Got 827 / 1000 correct (82.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.2580\n",
      "t = 200, loss = 0.1660\n",
      "t = 300, loss = 0.0335\n",
      "t = 400, loss = 0.1342\n",
      "t = 500, loss = 0.0571\n",
      "t = 600, loss = 0.0645\n",
      "t = 700, loss = 0.2900\n",
      "Checking accuracy on validation set\n",
      "Got 836 / 1000 correct (83.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1533\n",
      "t = 200, loss = 0.0644\n",
      "t = 300, loss = 0.2207\n",
      "t = 400, loss = 0.0498\n",
      "t = 500, loss = 0.1508\n",
      "t = 600, loss = 0.1231\n",
      "t = 700, loss = 0.1493\n",
      "Checking accuracy on validation set\n",
      "Got 823 / 1000 correct (82.30)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1358\n",
      "t = 200, loss = 0.0910\n",
      "t = 300, loss = 0.1865\n",
      "t = 400, loss = 0.0784\n",
      "t = 500, loss = 0.0559\n",
      "t = 600, loss = 0.0624\n",
      "t = 700, loss = 0.0773\n",
      "Checking accuracy on validation set\n",
      "Got 822 / 1000 correct (82.20)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1534\n",
      "t = 200, loss = 0.0897\n",
      "t = 300, loss = 0.0550\n",
      "t = 400, loss = 0.1290\n",
      "t = 500, loss = 0.0816\n",
      "t = 600, loss = 0.0925\n",
      "t = 700, loss = 0.2158\n",
      "Checking accuracy on validation set\n",
      "Got 825 / 1000 correct (82.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0919\n",
      "t = 200, loss = 0.0387\n",
      "t = 300, loss = 0.0565\n",
      "t = 400, loss = 0.0648\n",
      "t = 500, loss = 0.0941\n",
      "t = 600, loss = 0.0703\n",
      "t = 700, loss = 0.0511\n",
      "Checking accuracy on validation set\n",
      "Got 814 / 1000 correct (81.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0708\n",
      "t = 200, loss = 0.1286\n",
      "t = 300, loss = 0.0124\n",
      "t = 400, loss = 0.0410\n",
      "t = 500, loss = 0.0304\n",
      "t = 600, loss = 0.1591\n",
      "t = 700, loss = 0.2505\n",
      "Checking accuracy on validation set\n",
      "Got 836 / 1000 correct (83.60)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.0480\n",
      "t = 200, loss = 0.2254\n",
      "t = 300, loss = 0.0720\n",
      "t = 400, loss = 0.0795\n",
      "t = 500, loss = 0.0661\n",
      "t = 600, loss = 0.1565\n",
      "t = 700, loss = 0.1426\n",
      "Checking accuracy on validation set\n",
      "Got 824 / 1000 correct (82.40)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.1672\n",
      "t = 200, loss = 0.0720\n",
      "t = 300, loss = 0.0472\n",
      "t = 400, loss = 0.0361\n",
      "t = 500, loss = 0.0610\n",
      "t = 600, loss = 0.0620\n",
      "t = 700, loss = 0.5508\n",
      "Checking accuracy on validation set\n",
      "Got 816 / 1000 correct (81.60)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    train(model, loss_fn, optimizer, num_epochs=1)\n",
    "    acc = check_accuracy(model, loader_val)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        save_result(model, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n",
      "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/releases/download/mobilenetv2/cifar10_mobilenetv2_x1_4-3bbbd6e2.pt\" to /home/ubuntu/.cache/torch/hub/checkpoints/cifar10_mobilenetv2_x1_4-3bbbd6e2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1932e31f08bd47b581668f45b1c36db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_mobilenetv2_x1_4\", pretrained=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 1.5154\n",
      "t = 200, loss = 1.4907\n",
      "t = 300, loss = 1.6467\n",
      "t = 400, loss = 1.1686\n",
      "t = 500, loss = 1.3484\n",
      "t = 600, loss = 1.3046\n",
      "t = 700, loss = 1.5003\n",
      "Checking accuracy on validation set\n",
      "Got 409 / 1000 correct (40.90)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 1.2495\n",
      "t = 200, loss = 1.3425\n",
      "t = 300, loss = 1.3589\n",
      "t = 400, loss = 1.0412\n",
      "t = 500, loss = 1.3847\n",
      "t = 600, loss = 1.1598\n",
      "t = 700, loss = 1.2530\n",
      "Checking accuracy on validation set\n",
      "Got 535 / 1000 correct (53.50)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.9891\n",
      "t = 200, loss = 1.0844\n",
      "t = 300, loss = 1.2796\n",
      "t = 400, loss = 1.1204\n",
      "t = 500, loss = 1.1454\n",
      "t = 600, loss = 1.0564\n",
      "t = 700, loss = 1.3048\n",
      "Checking accuracy on validation set\n",
      "Got 497 / 1000 correct (49.70)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 1.0176\n",
      "t = 200, loss = 1.1006\n",
      "t = 300, loss = 1.1274\n",
      "t = 400, loss = 0.9473\n",
      "t = 500, loss = 1.1309\n",
      "t = 600, loss = 1.0435\n",
      "t = 700, loss = 1.1617\n",
      "Checking accuracy on validation set\n",
      "Got 520 / 1000 correct (52.00)\n",
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 0.9175\n",
      "t = 200, loss = 1.1661\n",
      "t = 300, loss = 1.2344\n",
      "t = 400, loss = 0.9296\n",
      "t = 500, loss = 1.1469\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train(model, loss_fn, optimizer, num_epochs=1)\n",
    "    acc = check_accuracy(model, loader_val)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"mobilenet_v2_cyf_x1_4.pt\")\n",
    "        save_result(model, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(model, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8929 / 10000 correct (89.29)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8929"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy_from_result(loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now that we've gotten a result we're happy with, we test our final model on the test set (which you should store in best_model).  This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "check_accuracy(best_model, loader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "538f48995952d5747fa2e8303f94856c2e9ff9985fb933039aabd93af1efe350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
